{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información de los tractos cerebrales\n",
    "tracts = {# LAs etiquetas de los tractos son: Tract, Side, \n",
    "    \"AF_L\": {\n",
    "        \"Tract\": \"arcuate Fasciculus\",\n",
    "        \"Side\": \"left\",\n",
    "    },\n",
    "    \"AF_R\": {\n",
    "        \"Tract\": \"arcuate Fasciculus\",\n",
    "        \"Side\": \"right\",\n",
    "    },\n",
    "    \"CC_Fr_1\": {\n",
    "        \"Tract\": \"Corpus Callosum\",\n",
    "        \"Side\": \"frontal lobe (most anterior part)\",\n",
    "    },\n",
    "    \"CC_Fr_2\": {\n",
    "        \"Tract\": \"Corpus Callosum\",\n",
    "        \"Side\": \"frontal lobe (most posterior part)\",\n",
    "    },\n",
    "    \"CC_Oc\": {\n",
    "        \"Tract\": \"Corpus Callosum\",\n",
    "        \"Side\": \"occipital lobe\",\n",
    "    },\n",
    "    \"CC_Pa\": {\n",
    "        \"Tract\": \"Corpus Callosum\",\n",
    "        \"Side\": \"parietal lobe\",\n",
    "    },\n",
    "    \"CC_Pr_Po\": {\n",
    "        \"Tract\": \"Corpus Callosum\",\n",
    "        \"Side\": \"pre/post central gyri\",\n",
    "    },\n",
    "    \"CG_L\": {\n",
    "        \"Tract\": \"cingulum\",\n",
    "        \"Side\": \"left\",\n",
    "    },\n",
    "    \"CG_R\": {\n",
    "        \"Tract\": \"cingulum\",\n",
    "        \"Side\": \"right\",\n",
    "    },\n",
    "    \"FAT_L\": {\n",
    "        \"Tract\": \"frontal aslant tract\",\n",
    "        \"Side\": \"left\",\n",
    "    },\n",
    "    \"FAT_R\": {\n",
    "        \"Tract\": \"frontal aslant tract\",\n",
    "        \"Side\": \"right\",\n",
    "    },\n",
    "    \"FPT_L\": {\n",
    "        \"Tract\": \"frontopontine tract\",\n",
    "        \"Side\": \"left\",\n",
    "    },\n",
    "    \"FPT_R\": {\n",
    "        \"Tract\": \"frontopontine tract\",\n",
    "        \"Side\": \"right\",\n",
    "    },\n",
    "    \"FX_L\": {\n",
    "        \"Tract\": \"fornix\",\n",
    "        \"Side\": \"left\",\n",
    "    },\n",
    "    \"FX_R\": {\n",
    "        \"Tract\": \"fornix\",\n",
    "        \"Side\": \"right\",\n",
    "    },\n",
    "    \"IFOF_L\": {\n",
    "        \"Tract\": \"inferior fronto-occipital fasciculus\",\n",
    "        \"Side\": \"left\",\n",
    "    },\n",
    "    \"IFOF_R\": {\n",
    "        \"Tract\": \"inferior fronto-occipital fasciculus\",\n",
    "        \"Side\": \"right\",\n",
    "    },\n",
    "    \"ILF_L\": {\n",
    "        \"Tract\": \"inferior longitudinal fasciculus\",\n",
    "        \"Side\": \"left\",\n",
    "    },\n",
    "    \"ILF_R\": {\n",
    "        \"Tract\": \"inferior longitudinal fasciculus\",\n",
    "        \"Side\": \"right\",\n",
    "    },\n",
    "    \"MCP\": {\n",
    "        \"Tract\": \"middle cerebellar peduncle\",\n",
    "        \"Side\": \"both\",\n",
    "    },\n",
    "    \"MdLF_L\": {\n",
    "        \"Tract\": \"middle longitudinal fascicle\",\n",
    "        \"Side\": \"left\",\n",
    "    },\n",
    "    \"MdLF_R\": {\n",
    "        \"Tract\": \"middle longitudinal fascicle\",\n",
    "        \"Side\": \"right\",\n",
    "    },\n",
    "    \"OR_ML_L\": {\n",
    "        \"Tract\": \"optic radiation and Meyer’s loop\",\n",
    "        \"Side\": \"left\",\n",
    "    },\n",
    "    \"OR_ML_R\": {\n",
    "        \"Tract\": \"optic radiation and Meyer’s loop\",\n",
    "        \"Side\": \"right\",\n",
    "    },\n",
    "    \"POPT_L\": {\n",
    "        \"Tract\": \"parieto-occipito pontine tract\",\n",
    "        \"Side\": \"left\",\n",
    "    },\n",
    "    \"POPT_R\": {\n",
    "        \"Tract\": \"parieto-occipito pontine tract\",\n",
    "        \"Side\": \"right\",\n",
    "    },\n",
    "    \"PYT_L\": {\n",
    "        \"Tract\": \"pyramidal tract\",\n",
    "        \"Side\": \"left\",\n",
    "    },\n",
    "    \"PYT_R\": {\n",
    "        \"Tract\": \"pyramidal tract\",\n",
    "        \"Side\": \"right\",\n",
    "    },\n",
    "    \"SLF_L\": {\n",
    "        \"Tract\": \"superior longitudinal fasciculus\",\n",
    "        \"Side\": \"left\",\n",
    "    },\n",
    "    \"SLF_R\": {\n",
    "        \"Tract\": \"superior longitudinal fasciculus\",\n",
    "        \"Side\": \"right\",\n",
    "    },\n",
    "    \"UF_L\": {\n",
    "        \"Tract\": \"uncinate fasciculus\",\n",
    "        \"Side\": \"left\",\n",
    "    },\n",
    "    \"UF_R\": {\n",
    "        \"Tract\": \"uncinate fasciculus\",\n",
    "        \"Side\": \"right\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_templates = [\n",
    "    \"Tract: {Tract}, Side: {Side}\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_templates = [\n",
    "    \"A fiber involved in [Disease] pathology.\",\n",
    "    \"This fiber, located in the [Region], is part of the [Tract] tract on the [Side] side, playing a key role in [Function]. It's often associated with [Disease] pathology.\",\n",
    "\"Identified on the [Side] hemisphere, this [Region]-originating fiber contributes to [Function], with significant implications for [Disease].\",\n",
    "\"The fiber in the [Region], belonging to the [Tract] tract, has been implicated in [Function], making it relevant in the study of [Disease].\",\n",
    "\"Spanning from the [Region] to other brain areas, this [Side] side fiber is crucial for [Function] and is linked to [Disease].\",\n",
    "\"This tractography-detected fiber on the [Side] side traverses the [Region], influencing [Function] and potentially affecting [Disease].\",\n",
    "\"A key component of the [Tract] tract, this fiber from the [Region] plays roles in [Function], with relevance to [Disease] research.\",\n",
    "\"In the [Region], a [Side]-side fiber within the [Tract] tract has been identified, associated with [Function] and [Disease].\",\n",
    "\"The [Tract] tract fiber, extending through the [Region] on the [Side] side, is instrumental for [Function] and [Disease] connections.\",\n",
    "\"This [Side] hemisphere fiber in the [Region], part of the [Tract], has functional implications for [Function] and associations with [Disease].\",\n",
    "\"Originating in the [Region] and part of the [Tract] tract, this fiber's role in [Function] is significant for understanding [Disease].\",\n",
    "\"The [Region]-based fiber, on the [Side] side and within the [Tract] tract, supports [Function], with potential implications for [Disease].\",\n",
    "\"A fiber in the [Region] of the [Side] hemisphere, contributing to the [Tract] tract, plays a critical part in [Function] and is related to [Disease].\",\n",
    "\"Found on the [Side] side, the fiber from the [Region], contributing to [Function], offers insights into [Disease].\",\n",
    "\"The [Region]'s fiber, integral to the [Tract] on the [Side] side, facilitates [Function] and has ties to [Disease].\",\n",
    "\"This fiber, traversing the [Region] and part of the [Tract] tract on the [Side] side, aids in [Function] and is linked to [Disease].\",\n",
    "\"Located in the [Region], this fiber of the [Tract] tract on the [Side] side is associated with [Function] and relevance to [Disease].\",\n",
    "\"A fiber in the [Tract] tract, spanning the [Region] on the [Side] side, supports [Function] and contributes to our understanding of [Disease].\",\n",
    "\"The fiber, found within the [Region] and part of the [Tract] on the [Side] side, plays a pivotal role in [Function] and is associated with [Disease].\",\n",
    "\"Spanning the [Region], this [Side]-side fiber within the [Tract] tract influences [Function] and has been studied in relation to [Disease].\",\n",
    "\"A critical part of the [Tract] tract, the fiber from the [Region] on the [Side] side, is instrumental in [Function] and studies on [Disease].\",\n",
    "\"Fiber in [Region] related to [Function].\",\n",
    "\"Located in the [Region], this fiber influences [Function].\",\n",
    "\"[Side] side [Tract] tract fiber, key for [Function].\",\n",
    "\"A [Region] fiber with implications for [Disease].\",\n",
    "\"Part of the [Tract], this fiber is crucial for [Function].\",\n",
    "\"[Region] [Side] fiber associated with [Function].\",\n",
    "\"Fiber contributing to [Function] found in [Region].\",\n",
    "\"In the [Region], a fiber relevant to [Disease] study.\",\n",
    "\"A [Tract] fiber on the [Side], important for [Function].\",\n",
    "\"[Region] fiber, a component of [Tract], affects [Function].\",\n",
    "\"Key [Region] fiber in [Function] research.\",\n",
    "\"On the [Side], a fiber impacting [Function].\",\n",
    "\"[Tract] tract fiber in [Region] linked to [Function].\",\n",
    "\"Fiber in [Region], significant for [Function] understanding.\",\n",
    "\"A [Side] hemisphere fiber associated with [Disease].\",\n",
    "\"Located in [Region], crucial for [Function] processes.\",\n",
    "\"Fiber from [Region] with a role in [Function].\",\n",
    "\"[Tract] tract [Side] fiber, relevant to [Disease].\",\n",
    "\"In [Region], fiber important for [Function].\",\n",
    "\"A [Region] [Tract] fiber, key in [Function] studies.\",\n",
    "\"Fiber in [Region] supports [Function].\",\n",
    "\"[Region] fiber, relevant for [Disease] insights.\",\n",
    "\"Essential for [Function]: fiber in [Region].\",\n",
    "\"[Tract] tract fiber on [Side] side, aids [Function].\",\n",
    "\"Fiber associated with [Function] in [Region].\",\n",
    "\"[Region]-origin fiber impacts [Function] processes.\",\n",
    "\"[Side] hemisphere fiber, linked to [Function].\",\n",
    "\"Fiber in [Region], pivotal for [Function] understanding.\",\n",
    "\"[Region] fiber, part of [Tract], influences [Function].\",\n",
    "\"Key to [Function], this [Region] fiber.\",\n",
    "\"Fiber on [Side] side, crucial for [Function].\",\n",
    "\"[Tract] fiber in [Region] with function in [Function].\",\n",
    "\"In [Region], fiber essential for [Function] research.\",\n",
    "\"Fiber from [Region], significant in [Disease] study.\",\n",
    "\"[Side] [Tract] fiber, important for [Function].\",\n",
    "\"[Region] fiber aids in understanding [Function].\",\n",
    "\"Linked to [Function], fiber in [Region].\",\n",
    "\"[Region] fiber, crucial in [Tract] for [Function].\",\n",
    "\"Fiber on [Side], relevant to [Function] and [Disease].\",\n",
    "\"Essential [Region] fiber for [Function] studies.\",\n",
    "\"[Tract]\",\n",
    "\"[Region]\",\n",
    "\"[Side]\",\n",
    "\"[Function]\",\n",
    "\"[Disease]\",\n",
    "\"[Tract], [Side]\"\n",
    "\"[Tract] located in the [Side] side\",\n",
    "\"[Region] fiber\",\n",
    "\"[Function] research\",\n",
    "\"[Disease] study\",\n",
    "\"Segment fibers located in the frontal area of the corpus callosum.\",\n",
    "\"Retrieve fibers from the left and right arcuate fasciculi.\",\n",
    "\"Classify fibers by their involvement in motor function within the [Side] hemisphere.\",\n",
    "\"Identify fibers in the [Region] that are part of the [Tract] tract.\",\n",
    "\"Segment fibers associated with language functions in the [Side] hemisphere.\",\n",
    "\"Extract fibers from the [Tract] tract that contribute to sensory processing.\",\n",
    "\"Locate fibers in the [Region] associated with cognitive functions.\",\n",
    "\"Distinguish fibers on the [Side] side of the brain that are implicated in emotional regulation.\",\n",
    "\"Filter fibers within the [Region] that play a role in visual processing.\",\n",
    "\"Classify fibers by their connection to the [Tract] tract in both the left and right hemispheres.\",\n",
    "\"Identify all fibers related to memory functions within the [Region].\",\n",
    "\"Segment fibers that are part of the limbic system in the [Side] hemisphere.\",\n",
    "\"Retrieve fibers associated with auditory processing from the [Tract] tract.\",\n",
    "\"Classify fibers in the [Region] based on their relevance to spatial awareness.\",\n",
    "\"Extract fibers on the [Side] side that are critical for executive functions.\",\n",
    "\"Locate fibers contributing to the somatosensory pathway in the [Tract] tract.\",\n",
    "\"Distinguish fibers in the [Region] involved in attention mechanisms.\",\n",
    "\"Filter fibers related to the [Tract] tract and associated with motor skill learning.\",\n",
    "\"Classify fibers by their involvement in the pain processing pathways in the [Region].\",\n",
    "\"Identify fibers on the [Side] side that connect to the [Tract], focusing on their role in language acquisition.\",\n",
    "\"Classify fibers by their involvement in motor functions within the [Side] hemisphere.\",\n",
    "\"Identify fibers in the [Region] associated with language processing.\",\n",
    "\"Extract fibers from the [Tract] that are implicated in cognitive functions.\",\n",
    "\"Select all fibers on the [Side] side that contribute to sensory pathways.\",\n",
    "\"Isolate fibers within the [Region] related to memory enhancement.\",\n",
    "\"Filter fibers by their connection to the [Disease] in the [Region].\",\n",
    "\"Group fibers based on their role in visual processing from the [Tract].\",\n",
    "\"Highlight fibers from the [Side] hemisphere that are part of the limbic system.\",\n",
    "\"Gather fibers associated with emotional regulation located in the [Region].\",\n",
    "# \"Segment fibers contributing to auditory functions in the [Tract].\",\n",
    "\"Catalog fibers by their involvement with [Function] in the [Region].\",\n",
    "\"Isolate fibers related to [Disease] recovery pathways in the [Tract].\",\n",
    "\"Classify fibers on the [Side] side that facilitate executive functions.\",\n",
    "\"Identify all fibers in the [Region] that are critical for spatial orientation.\",\n",
    "\"Retrieve fibers from the [Tract] known for their association with learning.\",\n",
    "\"Filter out fibers in the [Region] that are not associated with [Function].\",\n",
    "\"Highlight fibers within the [Side] hemisphere that have implications for [Disease].\",\n",
    "\"Extract and classify fibers by their role in the [Function] within the [Tract].\",\n",
    "\"Identify fibers in the [Tract] that are primarily involved in executive function tasks.\",\n",
    "\"Segment out fibers on the [Side] side that directly contribute to motor coordination.\",\n",
    "\"Classify all fibers found within the [Region] that play a role in emotional processing.\",\n",
    "# \"Retrieve fibers associated with [Function] from both the left and right [Tract].\",\n",
    "\"Isolate fibers in the [Region] that have been linked to advancements in [Disease] research.\",\n",
    "\"Catalog fibers from the [Tract] that are crucial for language comprehension.\",\n",
    "\"Highlight fibers within the [Region] associated with visuospatial skills.\",\n",
    "\"Filter fibers by their contribution to memory functions in the [Side] hemisphere.\",\n",
    "\"Group fibers based on their involvement in sensory processing within the [Tract].\",\n",
    "\"Select fibers in the [Region] that are known for their relation to [Disease].\",\n",
    "\"Extract fibers from the [Tract] that aid in auditory perception.\",\n",
    "\"Identify fibers on the [Side] side that support decision-making processes.\",\n",
    "\"Segment fibers contributing to [Function] in the [Region], especially those linked to [Disease].\",\n",
    "\"Classify fibers by their role in the regulation of [Function] within the [Tract].\",\n",
    "\"Isolate fibers in the [Region] that facilitate connectivity between hemispheres.\",\n",
    "\"Retrieve fibers associated with the [Side] hemisphere's [Function] pathways.\",\n",
    "\"Highlight fibers from the [Tract] involved in [Function], focusing on [Disease] implications.\",\n",
    "\"Filter out fibers in the [Region] not related to [Function].\",\n",
    "\"Group fibers by their critical role in [Function] within the [Tract].\",\n",
    "\"Catalog fibers on the [Side] side that are essential for [Function] and [Disease] management.\",\n",
    "\"Select all fibers contributing to [Function] in the [Region], with a focus on recovery from [Disease].\",\n",
    "\"Identify fibers within the [Tract] that enhance cognitive flexibility.\",\n",
    "\"Isolate fibers related to [Function] in the [Region], distinguishing between left and right sides.\",\n",
    "\"Classify fibers by their importance to [Function] in the context of [Disease] in the [Tract].\",\n",
    "\"Segment out fibers from the [Region] that are pivotal in neural plasticity studies.\",\n",
    "\"Classify fibers within the [Region] of the [Tract] on the [Side] side, focusing on their role in [Function] and relevance to [Disease].\",\n",
    "\"Identify and segment fibers in the [Region], part of the [Tract], that contribute to [Function], considering their impact on [Disease].\",\n",
    "\"Highlight all fibers from the [Region] in the [Tract] on the [Side], associated with [Function], and investigate their connection to [Disease].\",\n",
    "\"Isolate fibers located in the [Region] of the [Tract], on the [Side] hemisphere, involved in [Function], especially those linked to [Disease].\",\n",
    "\"Catalog fibers on the [Side] side of the [Tract] in [Region], focusing on their contribution to [Function] and their association with [Disease].\",\n",
    "\"Retrieve and classify fibers from the [Region] [Tract], on the [Side], that play a critical role in [Function], and assess their relevance to [Disease].\",\n",
    "\"Filter out fibers in the [Region] of the [Tract], on the [Side], not involved in [Function], to focus on those related to [Disease].\",\n",
    "\"Group all fibers within the [Region] [Tract], on the [Side] side, by their involvement in [Function] and potential implications for [Disease].\",\n",
    "\"Select fibers from the [Region] in the [Tract], on the [Side], that are essential for [Function], with a specific focus on [Disease].\",\n",
    "\"Identify fibers in the [Region] of the [Tract], on the [Side], that are key to [Function], with an emphasis on their links to [Disease].\",\n",
    "\"Highlight fibers from the [Region] within the [Tract] on the [Side], contributing to [Function], and explore their involvement in [Disease].\",\n",
    "\"Segment fibers in the [Region] [Tract], on the [Side], focusing on those critical to [Function] and evaluating their connection to [Disease].\",\n",
    "\"Classify fibers by their role in [Function] within the [Region] of the [Tract], considering the [Side] hemisphere and their relevance to [Disease].\",\n",
    "\"Extract fibers in the [Region] from the [Tract], on the [Side], with a significant role in [Function], particularly those associated with [Disease].\",\n",
    "\"Isolate and study fibers from the [Region] of the [Tract], on the [Side], involved in [Function], with a view to understanding their impact on [Disease].\",\n",
    "\"Catalog fibers on the [Side] of the [Tract] in [Region], emphasizing their involvement in [Function] and links to [Disease].\",\n",
    "\"Retrieve fibers from the [Region] [Tract], on the [Side], that enhance [Function], analyzing their association with [Disease].\",\n",
    "\"Filter fibers within the [Region] of the [Tract], on the [Side], focusing on their contribution to [Function] and analysis of [Disease] association.\",\n",
    "\"Group fibers by their functionality in [Function] within the [Region] of the [Tract], on the [Side], with a focus on [Disease] implications.\",\n",
    "\"Highlight all fibers from the [Region] in the [Tract] on the [Side], associated with [Function], for a comprehensive review of [Disease] connections.\",\n",
    "\"Segment out fibers in the [Region] of the [Tract], on the [Side], crucial for [Function], to evaluate their role in [Disease].\",\n",
    "\"Classify fibers based on their involvement in [Function] within the [Region] of the [Tract], on the [Side], with respect to [Disease].\",\n",
    "\"Identify fibers from the [Region] [Tract], on the [Side], playing a pivotal role in [Function], and their relation to [Disease].\",\n",
    "\"Isolate fibers in the [Region] of the [Tract], on the [Side], that are central to [Function], assessing their impact on [Disease].\",\n",
    "\"Extract and evaluate fibers from the [Region] [Tract], on the [Side], involved in [Function], with particular attention to [Disease].\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "def generate_caption(label:int):\n",
    "\n",
    "    caption = random.choice(caption_templates)\n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'caption_templates' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m tract_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorpus Callosum\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Puede ser cualquiera de los tractos definidos\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mgenerate_caption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtract_name\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[37], line 31\u001b[0m, in \u001b[0;36mgenerate_caption\u001b[1;34m(tract_name)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTract information not available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m tract \u001b[38;5;241m=\u001b[39m tract_info[tract_name]\n\u001b[1;32m---> 31\u001b[0m caption \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[43mcaption_templates\u001b[49m)\n\u001b[0;32m     32\u001b[0m caption_filled \u001b[38;5;241m=\u001b[39m caption\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Region]\u001b[39m\u001b[38;5;124m\"\u001b[39m, tract[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegion\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     33\u001b[0m caption_filled \u001b[38;5;241m=\u001b[39m caption_filled\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Tract]\u001b[39m\u001b[38;5;124m\"\u001b[39m, tract[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTract\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'caption_templates' is not defined"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Información de los tractos cerebrales\n",
    "tract_info = {\n",
    "    \"Corpus Callosum\": {\n",
    "        \"Region\": \"central\",\n",
    "        \"Tract\": \"Corpus Callosum\",\n",
    "        \"Side\": \"left and right hemispheres\",\n",
    "        \"Function\": \"facilitating communication between the hemispheres\",\n",
    "    },\n",
    "    \"Anterior Commissure\": {\n",
    "        \"Region\": \"central, anterior to the third ventricle\",\n",
    "        \"Tract\": \"Anterior Commissure\",\n",
    "        \"Side\": \"across the hemispheres\",\n",
    "        \"Function\": \"involved in pain, olfaction, and other functions\",\n",
    "    },\n",
    "    \"Internal Capsule\": {\n",
    "        \"Region\": \"central, adjacent to the thalamus\",\n",
    "        \"Tract\": \"Internal Capsule\",\n",
    "        \"Side\": \"connecting cortex with brainstem\",\n",
    "        \"Function\": \"carrying motor and sensory information\",\n",
    "    },\n",
    "    # Añadir el resto de la información de tractos aquí...\n",
    "}\n",
    "\n",
    "def generate_caption(tract_name):\n",
    "    if tract_name not in tract_info:\n",
    "        return \"Tract information not available.\"\n",
    "    \n",
    "    tract = tract_info[tract_name]\n",
    "    caption = random.choice(caption_templates)\n",
    "    caption_filled = caption.replace(\"[Region]\", tract[\"Region\"])\n",
    "    caption_filled = caption_filled.replace(\"[Tract]\", tract[\"Tract\"])\n",
    "    caption_filled = caption_filled.replace(\"[Side]\", tract[\"Side\"])\n",
    "    caption_filled = caption_filled.replace(\"[Function]\", tract[\"Function\"])\n",
    "    # Omitir [Disease] ya que no se proporciona información específica de enfermedades\n",
    "    return caption_filled\n",
    "\n",
    "# Ejemplo de uso:\n",
    "tract_name = \"Corpus Callosum\"  # Puede ser cualquiera de los tractos definidos\n",
    "for i in range(20):\n",
    "    print(generate_caption(tract_name))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity Search Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Similarity search\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "\n",
    "\n",
    "class Text_Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Text Encoder that wraps a pretrained transformer model for embedding extraction.\n",
    "\n",
    "    Attributes:\n",
    "        model (nn.Module): A pretrained model from Hugging Face's transformers.\n",
    "\n",
    "    Parameters:\n",
    "        pretrained_model_name_or_path (str): The name or path of the pretrained model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pretrained_model_name_or_path: str = 'distilbert-base-uncased') -> None:\n",
    "        \"\"\"\n",
    "        Initializes the TextEncoder with a pretrained model.\n",
    "\n",
    "        Parameters:\n",
    "            pretrained_model_name_or_path (str): The name or path to the pretrained model. Defaults to 'distilbert-base-uncased'.\n",
    "        \"\"\"\n",
    "        super(Text_Encoder, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(pretrained_model_name_or_path, torch_dtype=torch.float32)\n",
    "        \n",
    "        # Freeze the parameters of the pretrained model to prevent updates during training.\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, tokenized_text: dict) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the model to obtain embeddings.\n",
    "\n",
    "        Parameters:\n",
    "            tokenized_text (dict): Tokenized text input for the model.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The mean-pooled token embeddings.\n",
    "        \"\"\"\n",
    "        output = self.model(**tokenized_text)\n",
    "        # Generate embeddings by mean pooling.\n",
    "        cls_token_representations = self.meanpooling(output, tokenized_text['attention_mask'])\n",
    "        \n",
    "        return cls_token_representations\n",
    "    \n",
    "    def meanpooling(self, output: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Applies mean pooling on the model's output embeddings, taking the attention mask into account.\n",
    "\n",
    "        Parameters:\n",
    "            output (torch.Tensor): The output from the model.\n",
    "            mask (torch.Tensor): The attention mask for the input tokens.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The mean-pooled embeddings.\n",
    "        \"\"\"\n",
    "        embeddings = output[0]  # The first element of model_output contains all token embeddings.\n",
    "        mask = mask.unsqueeze(-1).expand(embeddings.size()).float()  # Adjust mask dimensions and type for multiplication.\n",
    "        # Compute mean-pooled embeddings.\n",
    "        return torch.sum(embeddings * mask, 1) / torch.clamp(mask.sum(1), min=1e-9)\n",
    "\n",
    "    \n",
    "class Graph_Encoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, out_channels):\n",
    "        super(Graph_Encoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)  # Capa adicional con la misma dimensión oculta\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, graph:Data):\n",
    "        x, edge_index, batch = graph.x, graph.edge_index, graph.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        out = global_mean_pool(x, batch)\n",
    "        return out\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,# The dimension of the input embeddings\n",
    "        projection_dim,# The dimension of the projected embeddings\n",
    "        dropout=.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Multimodal_Text_Graph_Model(nn.Module):\n",
    "    def __init__(self, hidden_size=768, projection_size=128, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.text_encoder = Text_Encoder()\n",
    "        self.text_projection_head = ProjectionHead(embedding_dim=hidden_size, projection_dim=projection_size)\n",
    "        self.graph_encoder = Graph_Encoder(4, 256, hidden_size)\n",
    "        self.graph_projection_head = ProjectionHead(embedding_dim=hidden_size, projection_dim=projection_size)\n",
    "        self.temperature = 1\n",
    "\n",
    "    \n",
    "    def forward(self, text, graph):\n",
    "        # Encode the text and project the representations\n",
    "        text_projections = self.text_encoder(text) # (batch_size, 768)\n",
    "        graph_projections = self.graph_encoder(graph) # (batch_size, 768)\n",
    "        text_projections = self.text_projection_head(text_projections) # (batch_size, 128)\n",
    "        graph_projections = self.graph_projection_head(graph_projections)# (batch_size, 128)\n",
    "        \n",
    "        # Calculating the Loss\n",
    "        logits = (text_projections @ graph_projections.T) / self.temperature\n",
    "        graphs_similarity = graph_projections @ graph_projections.T\n",
    "        texts_similarity = text_projections @ text_projections.T\n",
    "        targets = F.softmax(\n",
    "            (graphs_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        graphs_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (graphs_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        return loss.mean()\n",
    "    \n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 31])\n",
      "DataBatch(x=[750, 4], edge_index=[2, 1500], batch=[750], ptr=[26])\n",
      "tensor(12.2477, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from transformers import AutoTokenizer\n",
    "from torch_geometric.data import Batch\n",
    "import numpy as np\n",
    "\n",
    "# Asume que las clases de tu modelo ya están definidas como anteriormente\n",
    "\n",
    "# Tokenizador para el modelo de texto\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Función para generar datos de grafo aleatorios, igual que antes\n",
    "def generate_random_graph_data(num_nodes, num_node_features, num_edges):\n",
    "    x = torch.randn(num_nodes, num_node_features)  # Node features\n",
    "    edge_index = torch.randint(0, num_nodes, (2, num_edges))  # Edges\n",
    "    data = torch_geometric.data.Data(x=x, edge_index=edge_index)\n",
    "    return data\n",
    "\n",
    "# Generación de texto aleatorio (opcional)\n",
    "def generate_random_text(length=50):\n",
    "    words = [\"lorem\", \"ipsum\", \"dolor\", \"sit\", \"amet\", \"consectetur\", \"adipiscing\", \"elit\"]\n",
    "    text = ' '.join(np.random.choice(words, length))\n",
    "    return text\n",
    "\n",
    "# Preparación del batch de datos\n",
    "batch_size = 25  # Tamaño del lote\n",
    "graph_batch = []\n",
    "text_batch = []\n",
    "\n",
    "for _ in range(batch_size):\n",
    "    graph_data = generate_random_graph_data(num_nodes=30, num_node_features=4, num_edges=60)\n",
    "    text = generate_random_text(length=10)\n",
    "    graph_batch.append(graph_data)\n",
    "    text_batch.append(text)\n",
    "\n",
    "# Tokenizar el lote de texto\n",
    "text_batch_tokenized = tokenizer(text_batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Convertir el lote de grafos en un Batch de PyG\n",
    "graph_batch = Batch.from_data_list(graph_batch)\n",
    "\n",
    "print(text_batch_tokenized['input_ids'].shape)\n",
    "print(graph_batch)\n",
    "\n",
    "# Inicializar el modelo\n",
    "model = Multimodal_Text_Graph_Model()\n",
    "\n",
    "# Codificar el texto y el grafo\n",
    "loss = model(text_batch_tokenized, graph_batch)\n",
    "print(loss)  # (2, 128) - 2 representaciones proyectadas de 128 dimensiones cada una\n",
    "\n",
    "# # Mostrar los logits en un mapa de calor\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# sns.heatmap(logits.detach().numpy(), cmap='viridis', annot=True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "\n",
    "class GraphEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode graphs (torch_geometric) to a fixed size vector.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_dim, out_channels):\n",
    "        super(GraphEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # First GCN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        # Second GCN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.relu(x)\n",
    "        # Global mean pooling to aggregate node features into a single vector per graph\n",
    "        out = global_mean_pool(x, batch)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encode text to a fixed size vector using pre-trained sentence embeddings.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_name: str\n",
    "        Name of the pre-trained model to use for encoding text.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        super(TextEncoder, self).__init__()\n",
    "        # Load the pre-trained model\n",
    "        self.model = torch.hub.load('sentence-transformers', model_name)\n",
    "    \n",
    "    def forward(self, text):\n",
    "        \"\"\"\n",
    "        Forward pass of the encoder.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        text: str\n",
    "            Input text to encode.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        out: torch.Tensor\n",
    "            Encoded text representation with shape [1, embedding_dim].\n",
    "        \"\"\"\n",
    "        # Encode the text\n",
    "        out = self.model.encode(text, convert_to_tensor=True)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, self, temperature, image_embedding, text_embedding):\n",
    "        super(Model, self).__init__()\n",
    "        # Text encoder model using sentence embedding from huggingface transformers\n",
    "        self.text_encoder_module = torch.nn.Identity()\n",
    "        # Graph encoder network using \n",
    "        self.graph_encoder_module = torch_geometric.nn.GCNConv(16, 32)\n",
    "\n",
    "        # Add your model layers and configurations here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # La entrada x contiene un grafo y un texto\n",
    "        graph = x['graph']\n",
    "        text = x['text']\n",
    "        # Encode the graph\n",
    "        graph_encoded = self.graph_encoder_module(graph.x, graph.edge_index, graph.batch)\n",
    "        # Encode the text\n",
    "        text_encoded = self.text_encoder_module(text)\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        temperature=CFG.temperature,\n",
    "        image_embedding=CFG.image_embedding,\n",
    "        text_embedding=CFG.text_embedding,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.image_encoder = ImageEncoder()\n",
    "        self.text_encoder = TextEncoder()\n",
    "        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n",
    "        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # Getting Image and Text Features\n",
    "        image_features = self.image_encoder(batch[\"image\"])\n",
    "        text_features = self.text_encoder(\n",
    "            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "        # Getting Image and Text Embeddings (with same dimension)\n",
    "        image_embeddings = self.image_projection(image_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        # Calculating the Loss\n",
    "        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n",
    "        images_similarity = image_embeddings @ image_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "        targets = F.softmax(\n",
    "            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = cross_entropy(logits, targets, reduction='none')\n",
    "        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n",
    "        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "def cross_entropy(preds, targets, reduction='none'):\n",
    "    log_softmax = nn.LogSoftmax(dim=-1)\n",
    "    loss = (-targets * log_softmax(preds)).sum(1)\n",
    "    if reduction == \"none\":\n",
    "        return loss\n",
    "    elif reduction == \"mean\":\n",
    "        return loss.mean()        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm_prg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
