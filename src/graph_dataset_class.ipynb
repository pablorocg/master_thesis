{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset de torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Compose, NormalizeFeatures\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightningDataModule\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Dataset\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Batch, Data\n",
    "import random\n",
    "from torch_geometric.transforms import Compose\n",
    "from transformers import AutoTokenizer\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import Batch as GeoBatch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "\n",
    "class MaxMinNormalization(BaseTransform):\n",
    "    def __init__(self, max_values=None, min_values=None):\n",
    "        \"\"\"\n",
    "        Initialize the normalization transform with optional max and min values.\n",
    "        If not provided, they should be computed from the dataset.\n",
    "        \"\"\"\n",
    "        self.max_values = max_values if max_values is not None else torch.tensor([76.03170776367188, 77.9359130859375, 88.72427368164062], dtype=torch.float)\n",
    "        self.min_values = min_values if min_values is not None else torch.tensor([-73.90082550048828, -112.23554992675781, -79.38320922851562], dtype=torch.float)\n",
    "\n",
    "    def __call__(self, data: Data) -> Data:\n",
    "        \"\"\"\n",
    "        Apply min-max normalization to the node features.\n",
    "        \"\"\"\n",
    "        data.x = (data.x - self.min_values) / (self.max_values - self.min_values)\n",
    "        return data\n",
    "    \n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "import random\n",
    "\n",
    "\n",
    "class MyLazyDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None):\n",
    "        super(MyLazyDataset, self).__init__(root, transform, pre_transform)\n",
    "        self.transform = Compose([MaxMinNormalization()])\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        # De manera similar, lista los archivos en el directorio 'processed'\n",
    "        return os.listdir(os.path.join(self.root, 'processed'))\n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        # Lista los archivos en el directorio 'raw'\n",
    "        return os.listdir(os.path.join(self.root, 'raw'))\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        \n",
    "        subject = self.processed_file_names[idx]# Seleccionar un sujeto\n",
    "        graphs = torch.load(os.path.join(self.processed_dir, subject))\n",
    "        if self.transform:\n",
    "            graphs = self.transform(graphs)\n",
    "\n",
    "        return graphs\n",
    "\n",
    "\n",
    "# Uso de tu clase de conjunto de datos\n",
    "dataset = MyLazyDataset(root=r'C:\\Users\\pablo\\GitHub\\tfm_prg\\tractoinferno_graphs\\testset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando sujeto...\n",
      "Sujeto cargado\n",
      "Aplicando transformaciones...\n",
      "Transformaciones aplicadas\n"
     ]
    }
   ],
   "source": [
    "first_element = dataset[0] # Esto carga el primer ejemplo de tu conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1996,  6143,  ...,     0,     0,     0],\n",
      "        [  101,  6583,  5737,  ...,     0,     0,     0],\n",
      "        [  101, 11131,  1996,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  1037,  4012,  ...,     0,     0,     0],\n",
      "        [  101,  1996,  2523,  ...,     0,     0,     0],\n",
      "        [  101,  6459,  1997,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import Batch as GeoBatch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "def collate_function(batch):\n",
    "    \"\"\"Funcion para el DataLoader\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    TRACT_LIST = {\n",
    "        'AF_L': {'id': 0, 'tract': 'arcuate fasciculus', 'side' : 'left', 'type': 'association'},\n",
    "        'AF_R': {'id': 1, 'tract': 'arcuate fasciculus','side' : 'right', 'type': 'association'},\n",
    "        'CC_Fr_1': {'id': 2, 'tract': 'corpus callosum, frontal lobe', 'side' : 'most anterior part of the frontal lobe', 'type': 'commissural'},\n",
    "        'CC_Fr_2': {'id': 3, 'tract': 'corpus callosum, frontal lobe', 'side' : 'most posterior part of the frontal lobe','type': 'commissural'},\n",
    "        'CC_Oc': {'id': 4, 'tract': 'corpus callosum, occipital lobe', 'side' : 'central', 'type': 'commissural'},\n",
    "        'CC_Pa': {'id': 5, 'tract': 'corpus callosum, parietal lobe', 'side' : 'central', 'type': 'commissural'},\n",
    "        'CC_Pr_Po': {'id': 6, 'tract': 'corpus callosum, pre/post central gyri', 'side' : 'central', 'type': 'commissural'},\n",
    "        'CG_L': {'id': 7, 'tract': 'cingulum', 'side' : 'left', 'type': 'association'},\n",
    "        'CG_R': {'id': 8, 'tract': 'cingulum', 'side' : 'right', 'type': 'association'},\n",
    "        'FAT_L': {'id': 9, 'tract': 'frontal aslant tract', 'side' : 'left', 'type': 'association'},\n",
    "        'FAT_R': {'id': 10, 'tract': 'frontal aslant tract', 'side' : 'right', 'type': 'association'},\n",
    "        'FPT_L': {'id': 11, 'tract': 'fronto-pontine tract', 'side' : 'left', 'type': 'association'},\n",
    "        'FPT_R': {'id': 12, 'tract': 'fronto-pontine tract', 'side' : 'right', 'type': 'association'},\n",
    "        'FX_L': {'id': 13, 'tract': 'fornix', 'side' : 'left', 'type': 'commissural'},\n",
    "        'FX_R': {'id': 14, 'tract': 'fornix', 'side' : 'right', 'type': 'commissural'},\n",
    "        'IFOF_L': {'id': 15, 'tract': 'inferior fronto-occipital fasciculus', 'side' : 'left', 'type': 'association'},\n",
    "        'IFOF_R': {'id': 16, 'tract': 'inferior fronto-occipital fasciculus', 'side' : 'right', 'type': 'association'},\n",
    "        'ILF_L': {'id': 17, 'tract': 'inferior longitudinal fasciculus', 'side' : 'left', 'type': 'association'},\n",
    "        'ILF_R': {'id': 18, 'tract': 'inferior longitudinal fasciculus', 'side' : 'right', 'type': 'association'},\n",
    "        'MCP': {'id': 19, 'tract': 'middle cerebellar peduncle', 'side' : 'central', 'type': 'commissural'},\n",
    "        'MdLF_L': {'id': 20, 'tract': 'middle longitudinal fasciculus', 'side' : 'left', 'type': 'association'},\n",
    "        'MdLF_R': {'id': 21, 'tract': 'middle longitudinal fasciculus', 'side' : 'right', 'type': 'association'},\n",
    "        'OR_ML_L': {'id': 22, 'tract': 'optic radiation, Meyer loop', 'side' : 'left', 'type': 'projection'},\n",
    "        'OR_ML_R': {'id': 23, 'tract': 'optic radiation, Meyer loop', 'side' : 'right', 'type': 'projection'},\n",
    "        'POPT_L': {'id': 24, 'tract': 'pontine crossing tract', 'side' : 'left', 'type': 'commissural'},\n",
    "        'POPT_R': {'id': 25, 'tract': 'pontine crossing tract', 'side' : 'right', 'type': 'commissural'},\n",
    "        'PYT_L': {'id': 26, 'tract': 'pyramidal tract', 'side' : 'left', 'type': 'projection'},\n",
    "        'PYT_R': {'id': 27, 'tract': 'pyramidal tract', 'side' : 'right', 'type': 'projection'},\n",
    "        'SLF_L': {'id': 28, 'tract': 'superior longitudinal fasciculus', 'side' : 'left', 'type': 'association'},\n",
    "        'SLF_R': {'id': 29, 'tract': 'superior longitudinal fasciculus', 'side' : 'right', 'type': 'association'},\n",
    "        'UF_L': {'id': 30, 'tract': 'uncinate fasciculus', 'side' : 'left', 'type': 'association'},\n",
    "        'UF_R': {'id': 31, 'tract': 'uncinate fasciculus', 'side' : 'right', 'type': 'association'}\n",
    "    }\n",
    "\n",
    "    LABELS = {value[\"id\"]: key for key, value in TRACT_LIST.items()}# Diccionario id -> Etiqueta\n",
    "    caption_templates = [\n",
    "            \"A {type} fiber\",\n",
    "            \"A {type} fiber on the {side} side\",\n",
    "            \"{type} fiber on the {side} side\",\n",
    "            \"A {type} fiber of the {tract}\",\n",
    "            \"{type} fiber of the {tract}\",\n",
    "            \"A {type} fiber of the {tract} on the {side} side\",\n",
    "            \"{type} fiber of the {tract} on the {side} side\",\n",
    "            \"{side} side\",\n",
    "            \"{tract} tract\",\n",
    "            \"{type} fiber\",\n",
    "            \"The {type} fiber located in the {tract} tract\",\n",
    "            \"This is a {type} fiber found on the {side} hemisphere\",\n",
    "            \"Detailed view of a {type} fiber within the {tract}\",\n",
    "            \"Observation of the {type} fiber, prominently on the {side} side\",\n",
    "            \"The {tract} tract's remarkable {type} fiber\",\n",
    "            \"Characteristics of a {type} fiber in the {tract} region\",\n",
    "            \"Notable {type} fiber on the {side} hemisphere of the {tract}\",\n",
    "            \"Insight into the {type} fiber's structure on the {side} side\",\n",
    "            \"Exploring the complexity of the {type} fiber in the {tract}\",\n",
    "            \"The anatomy of a {type} fiber on the {side} hemisphere\",\n",
    "            \"The {tract} tract featuring a {type} fiber\",\n",
    "            \"A comprehensive look at the {type} fiber, {side} orientation\",\n",
    "            \"A closer look at the {type} fiber's path in the {tract}\",\n",
    "            \"Unveiling the {type} fiber's role in the {tract} tract\",\n",
    "            \"Decoding the structure of the {type} fiber on the {side}\",\n",
    "            \"Highlighting the {type} fiber's significance in the {tract}\",\n",
    "            \"The {type} fiber: A journey through the {tract} on the {side}\",\n",
    "            \"A deep dive into the {type} fiber's dynamics in the {tract}\",\n",
    "            \"The {type} fiber's contribution to {tract} tract functionality\",\n",
    "            \"Mapping the {type} fiber's trajectory in the {tract} on the {side} side\",\n",
    "            \"Navigating the intricate pathways of the {type} fiber within the {tract}\",\n",
    "            \"The interplay of {type} fibers across the {side} hemisphere\",\n",
    "            \"Traversing the {tract} with a {type} fiber\",\n",
    "            \"The pivotal role of the {type} fiber in connecting the {tract}\",\n",
    "            \"Showcasing the unique texture of {type} fibers in the {tract}\",\n",
    "            \"Zooming in on the {type} fiber's impact on the {side} hemisphere\",\n",
    "            \"The {type} fiber in the {tract}\",\n",
    "            \"The {type} fiber as a conduit in the {tract} on the {side} side\",\n",
    "            \"The {type} fiber's architectural marvel within the {tract}\",\n",
    "            \"A journey alongside the {type} fiber through the {tract}\",\n",
    "            \"The harmonious structure of the {type} fiber in the {tract}\",\n",
    "            \"Unraveling the secrets of the {type} fiber in the {tract} tract\",\n",
    "            \"The {type} fiber: A key player in {tract} dynamics\",\n",
    "            \"Envisioning the {type} fiber's pathway in the {tract}\",\n",
    "            \"The strategic placement of the {type} fiber in the {tract}\",\n",
    "            \"Illuminating the {type} fiber's route through the {tract}\",\n",
    "            \"The {type} fiber: An essential bridge within the {tract}\",\n",
    "            \"Deciphering the network of {type} fibers in the {tract}\",\n",
    "            \"Exploring the synergy between {type} fibers and the {tract}\",\n",
    "            \"The {type} fiber's vital link in the neural network of the {tract}\"\n",
    "        ]\n",
    "\n",
    "    # Extraer los labels de todos los grafos en el lote\n",
    "    labels = [graph.y.item() for graph in batch]  # Asumiendo que `y` es el tensor de labels\n",
    "    \n",
    "    # Recuperar y tokenizar todos los captions necesarios en una sola llamada\n",
    "    captions = [random.choice(caption_templates).format(**TRACT_LIST[LABELS[label]]) for label in labels]\n",
    "    tokenized_texts_batch = tokenizer(captions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Devolver el lote procesado. No es necesario devolver tokenized_texts_batch por separado\n",
    "    return batch, tokenized_texts_batch # grafos, {'input_ids': padded_input_ids, 'attention_mask': padded_attention_masks}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for subject in dataset:\n",
    "    for graph_batch, text_batch in DataLoader(subject, batch_size=512, shuffle=True, num_workers=0, drop_last=True, collate_fn=collate_function):\n",
    "        print(text_batch)\n",
    "        break\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando sujeto...\n",
      "Sujeto cargado\n",
      "Aplicando transformaciones...\n",
      "Transformaciones aplicadas\n",
      "('x', tensor([[0.4940, 0.5919, 0.4754],\n",
      "        [0.4942, 0.5919, 0.4755],\n",
      "        [0.4943, 0.5919, 0.4755],\n",
      "        ...,\n",
      "        [0.4981, 0.5919, 0.4762],\n",
      "        [0.4982, 0.5919, 0.4762],\n",
      "        [0.4984, 0.5918, 0.4762]]))\n",
      "['x', tensor([[0.4940, 0.5919, 0.4754],\n",
      "        [0.4942, 0.5919, 0.4755],\n",
      "        [0.4943, 0.5919, 0.4755],\n",
      "        ...,\n",
      "        [0.4981, 0.5919, 0.4762],\n",
      "        [0.4982, 0.5919, 0.4762],\n",
      "        [0.4984, 0.5918, 0.4762]])]\n",
      "('edge_index', tensor([[       0,        1,        2,  ..., 24849858, 24849859, 24849860],\n",
      "        [       1,        2,        3,  ..., 24849857, 24849858, 24849859]]))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 1 in argument 0, but got str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[0;32m     51\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m DataLoader(batch, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;66;03m#, transform=transform\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:55\u001b[0m, in \u001b[0;36mCollater.collate_fn\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, OnDiskDataset):\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mmulti_get(batch))\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:34\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Batch\u001b[38;5;241m.\u001b[39mfrom_data_list(\n\u001b[0;32m     29\u001b[0m         batch,\n\u001b[0;32m     30\u001b[0m         follow_batch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_batch,\n\u001b[0;32m     31\u001b[0m         exclude_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexclude_keys,\n\u001b[0;32m     32\u001b[0m     )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_collate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, TensorFrame):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch_frame\u001b[38;5;241m.\u001b[39mcat(batch, along\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 1 in argument 0, but got str"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "class MaxMinNormalization(BaseTransform):\n",
    "    def __init__(self, max_values=None, min_values=None):\n",
    "        \"\"\"\n",
    "        Initialize the normalization transform with optional max and min values.\n",
    "        If not provided, they should be computed from the dataset.\n",
    "        \"\"\"\n",
    "        self.max_values = max_values if max_values is not None else torch.tensor([76.03170776367188, 77.9359130859375, 88.72427368164062], dtype=torch.float)\n",
    "        self.min_values = min_values if min_values is not None else torch.tensor([-73.90082550048828, -112.23554992675781, -79.38320922851562], dtype=torch.float)\n",
    "\n",
    "    def __call__(self, data: Data) -> Data:\n",
    "        \"\"\"\n",
    "        Apply min-max normalization to the node features.\n",
    "        \"\"\"\n",
    "        data.x = (data.x - self.min_values) / (self.max_values - self.min_values)\n",
    "        return data\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "#Entrenar el modelo utilizando la gpu y tensorboard\n",
    "\n",
    "# print(f\"Using device: {device}\")\n",
    "# model.to(device)\n",
    "# for epoch in range(2):\n",
    "#     for i, (graph_data, text_data) in enumerate(dataloader):\n",
    "#         graph_data = graph_data.to(device)\n",
    "#         text_data = {key: val.to(device) for key, val in text_data.items()}\n",
    "#         loss = model(graph_data, text_data)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         writer.add_scalar('Loss/train', loss, epoch * len(dataloader) + i)\n",
    "#         print(f\"\\r Epoch {epoch}, Iteration {i}, Loss {loss}\")\n",
    "# writer.flush()\n",
    "# writer.close()\n",
    "\n",
    "# Crear un transform customizado para generar captions\n",
    "# transform = Compose([MaxMinNormalization()])\n",
    "\n",
    "# Crear un DataLoader que aplica el transform customizado\n",
    "for subject in dataset:\n",
    "    for batch in subject:\n",
    "        print(batch)\n",
    "        dataloader = DataLoader(batch, batch_size=2048, shuffle=True, collate_fn=)#, transform=transform\n",
    "        for batch in dataloader:\n",
    "            print(batch)\n",
    "            \n",
    "    break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formato del dataset\n",
    "\n",
    "# grafos (lista de Data), captions (lista de textos)\n",
    "\n",
    "\n",
    "# Codigo para generar un batch \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando sujeto...\n",
      "Sujeto cargado\n",
      "Aplicando normalización...\n",
      "Normalizacion aplicada\n",
      "Generando captions...\n",
      "Captions generadas\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'stores'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataListLoader, DataLoader\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subject \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(DataLoader(subject, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:55\u001b[0m, in \u001b[0;36mCollater.collate_fn\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, OnDiskDataset):\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mmulti_get(batch))\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:48\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(elem)(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader found invalid type: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(elem)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:48\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(elem)(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader found invalid type: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(elem)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:28\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     26\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch_geometric\\data\\batch.py:93\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[1;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data_list: List[BaseData],\n\u001b[0;32m     83\u001b[0m                    follow_batch: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     84\u001b[0m                    exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m    Python list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    :obj:`follow_batch`.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)\n\u001b[0;32m    103\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch_geometric\\data\\collate.py:54\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     52\u001b[0m key_to_stores \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_list:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstores\u001b[49m:\n\u001b[0;32m     55\u001b[0m         key_to_stores[store\u001b[38;5;241m.\u001b[39m_key]\u001b[38;5;241m.\u001b[39mappend(store)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# With this, we iterate over each list of storage objects and recursively\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# collate all its attributes into a unified representation:\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#   elements as attributes that got incremented need to be decremented\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m#   while separating to obtain original values.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'stores'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataListLoader, DataLoader\n",
    "\n",
    "# def custom_collate_fn(batch):\n",
    "#     graphs = [item['graph'] for item in batch]\n",
    "#     input_ids = [item['text']['input_ids'].squeeze(0) for item in batch]\n",
    "#     attention_masks = [item['text']['attention_mask'].squeeze(0) for item in batch]\n",
    "#     padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "#     padded_attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "#     batched_graphs = GeoBatch.from_data_list(graphs)\n",
    "#     return batched_graphs, {'input_ids': padded_input_ids, 'attention_mask': padded_attention_masks}\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv, BatchNorm\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class GraphClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GraphClassifier, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 32)\n",
    "        self.conv3 = GCNConv(32, 256)\n",
    "        self.conv4 = GCNConv(256, 512)\n",
    "        self.fc1 = torch.nn.Linear(512, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 64)\n",
    "        self.fc3 = torch.nn.Linear(64, 32)\n",
    "        self.fc = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = BatchNorm(x.size()[1])(x)\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = BatchNorm(x.size()[1])(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = BatchNorm(x.size()[1])(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = BatchNorm(x.size()[1])(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = BatchNorm(x.size()[1])(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = BatchNorm(x.size()[1])(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = BatchNorm(x.size()[1])(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = BatchNorm(x.size()[1])(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = GraphClassifier(3, 32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "    for subject in dataset:\n",
    "        for batch in tqdm(DataLoader(subject, batch_size=128, shuffle=True)):\n",
    "            data = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = F.nll_loss(out, data.y)\n",
    "            # print(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Epoch: {:03d}, Loss: {:.5f}'.format(epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre el conjunto de datos\n",
    "for data in dataset:\n",
    "    for batch in DataListLoader(data, batch_size=128, shuffle=True):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader de torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch.shape)\n",
    "    print(batch.num_graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddCaptionTransform(BaseTransform):\n",
    "    def __init__(self, tokenize_data=True):\n",
    "        \"\"\"\n",
    "        Initialize the transform with the tokenizer and the tract list.\n",
    "        \"\"\"\n",
    "        self.tokenize_data = tokenize_data\n",
    "\n",
    "        if self.tokenize_data:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        else:\n",
    "            self.tokenizer = None\n",
    "\n",
    "\n",
    "        # Your TRACT_LIST and caption_templates can be defined here\n",
    "        self.TRACT_LIST = {\n",
    "            'AF_L': {'id': 0, 'tract': 'arcuate fasciculus', 'side' : 'left', 'type': 'association'},\n",
    "            'AF_R': {'id': 1, 'tract': 'arcuate fasciculus','side' : 'right', 'type': 'association'},\n",
    "            'CC_Fr_1': {'id': 2, 'tract': 'corpus callosum, frontal lobe', 'side' : 'most anterior part of the frontal lobe', 'type': 'commissural'},\n",
    "            'CC_Fr_2': {'id': 3, 'tract': 'corpus callosum, frontal lobe', 'side' : 'most posterior part of the frontal lobe','type': 'commissural'},\n",
    "            'CC_Oc': {'id': 4, 'tract': 'corpus callosum, occipital lobe', 'side' : 'central', 'type': 'commissural'},\n",
    "            'CC_Pa': {'id': 5, 'tract': 'corpus callosum, parietal lobe', 'side' : 'central', 'type': 'commissural'},\n",
    "            'CC_Pr_Po': {'id': 6, 'tract': 'corpus callosum, pre/post central gyri', 'side' : 'central', 'type': 'commissural'},\n",
    "            'CG_L': {'id': 7, 'tract': 'cingulum', 'side' : 'left', 'type': 'association'},\n",
    "            'CG_R': {'id': 8, 'tract': 'cingulum', 'side' : 'right', 'type': 'association'},\n",
    "            'FAT_L': {'id': 9, 'tract': 'frontal aslant tract', 'side' : 'left', 'type': 'association'},\n",
    "            'FAT_R': {'id': 10, 'tract': 'frontal aslant tract', 'side' : 'right', 'type': 'association'},\n",
    "            'FPT_L': {'id': 11, 'tract': 'fronto-pontine tract', 'side' : 'left', 'type': 'association'},\n",
    "            'FPT_R': {'id': 12, 'tract': 'fronto-pontine tract', 'side' : 'right', 'type': 'association'},\n",
    "            'FX_L': {'id': 13, 'tract': 'fornix', 'side' : 'left', 'type': 'commissural'},\n",
    "            'FX_R': {'id': 14, 'tract': 'fornix', 'side' : 'right', 'type': 'commissural'},\n",
    "            'IFOF_L': {'id': 15, 'tract': 'inferior fronto-occipital fasciculus', 'side' : 'left', 'type': 'association'},\n",
    "            'IFOF_R': {'id': 16, 'tract': 'inferior fronto-occipital fasciculus', 'side' : 'right', 'type': 'association'},\n",
    "            'ILF_L': {'id': 17, 'tract': 'inferior longitudinal fasciculus', 'side' : 'left', 'type': 'association'},\n",
    "            'ILF_R': {'id': 18, 'tract': 'inferior longitudinal fasciculus', 'side' : 'right', 'type': 'association'},\n",
    "            'MCP': {'id': 19, 'tract': 'middle cerebellar peduncle', 'side' : 'central', 'type': 'commissural'},\n",
    "            'MdLF_L': {'id': 20, 'tract': 'middle longitudinal fasciculus', 'side' : 'left', 'type': 'association'},\n",
    "            'MdLF_R': {'id': 21, 'tract': 'middle longitudinal fasciculus', 'side' : 'right', 'type': 'association'},\n",
    "            'OR_ML_L': {'id': 22, 'tract': 'optic radiation, Meyer loop', 'side' : 'left', 'type': 'projection'},\n",
    "            'OR_ML_R': {'id': 23, 'tract': 'optic radiation, Meyer loop', 'side' : 'right', 'type': 'projection'},\n",
    "            'POPT_L': {'id': 24, 'tract': 'pontine crossing tract', 'side' : 'left', 'type': 'commissural'},\n",
    "            'POPT_R': {'id': 25, 'tract': 'pontine crossing tract', 'side' : 'right', 'type': 'commissural'},\n",
    "            'PYT_L': {'id': 26, 'tract': 'pyramidal tract', 'side' : 'left', 'type': 'projection'},\n",
    "            'PYT_R': {'id': 27, 'tract': 'pyramidal tract', 'side' : 'right', 'type': 'projection'},\n",
    "            'SLF_L': {'id': 28, 'tract': 'superior longitudinal fasciculus', 'side' : 'left', 'type': 'association'},\n",
    "            'SLF_R': {'id': 29, 'tract': 'superior longitudinal fasciculus', 'side' : 'right', 'type': 'association'},\n",
    "            'UF_L': {'id': 30, 'tract': 'uncinate fasciculus', 'side' : 'left', 'type': 'association'},\n",
    "            'UF_R': {'id': 31, 'tract': 'uncinate fasciculus', 'side' : 'right', 'type': 'association'}\n",
    "        }\n",
    "\n",
    "        self.LABELS = {value[\"id\"]: key for key, value in self.TRACT_LIST.items()}# Diccionario id -> Etiqueta\n",
    "\n",
    "        self.caption_templates = [\n",
    "            \"A {type} fiber\",\n",
    "            \"A {type} fiber on the {side} side\",\n",
    "            \"{type} fiber on the {side} side\",\n",
    "            \"A {type} fiber of the {tract}\",\n",
    "            \"{type} fiber of the {tract}\",\n",
    "            \"A {type} fiber of the {tract} on the {side} side\",\n",
    "            \"{type} fiber of the {tract} on the {side} side\",\n",
    "            \"{side} side\",\n",
    "            \"{tract} tract\",\n",
    "            \"{type} fiber\",\n",
    "            \"The {type} fiber located in the {tract} tract\",\n",
    "            \"This is a {type} fiber found on the {side} hemisphere\",\n",
    "            \"Detailed view of a {type} fiber within the {tract}\",\n",
    "            \"Observation of the {type} fiber, prominently on the {side} side\",\n",
    "            \"The {tract} tract's remarkable {type} fiber\",\n",
    "            \"Characteristics of a {type} fiber in the {tract} region\",\n",
    "            \"Notable {type} fiber on the {side} hemisphere of the {tract}\",\n",
    "            \"Insight into the {type} fiber's structure on the {side} side\",\n",
    "            \"Exploring the complexity of the {type} fiber in the {tract}\",\n",
    "            \"The anatomy of a {type} fiber on the {side} hemisphere\",\n",
    "            \"The {tract} tract featuring a {type} fiber\",\n",
    "            \"A comprehensive look at the {type} fiber, {side} orientation\",\n",
    "            \"A closer look at the {type} fiber's path in the {tract}\",\n",
    "            \"Unveiling the {type} fiber's role in the {tract} tract\",\n",
    "            \"Decoding the structure of the {type} fiber on the {side}\",\n",
    "            \"Highlighting the {type} fiber's significance in the {tract}\",\n",
    "            \"The {type} fiber: A journey through the {tract} on the {side}\",\n",
    "            \"A deep dive into the {type} fiber's dynamics in the {tract}\",\n",
    "            \"The {type} fiber's contribution to {tract} tract functionality\",\n",
    "            \"Mapping the {type} fiber's trajectory in the {tract} on the {side} side\",\n",
    "            \"Navigating the intricate pathways of the {type} fiber within the {tract}\",\n",
    "            \"The interplay of {type} fibers across the {side} hemisphere\",\n",
    "            \"Traversing the {tract} with a {type} fiber\",\n",
    "            \"The pivotal role of the {type} fiber in connecting the {tract}\",\n",
    "            \"Showcasing the unique texture of {type} fibers in the {tract}\",\n",
    "            \"Zooming in on the {type} fiber's impact on the {side} hemisphere\",\n",
    "            \"The {type} fiber in the {tract}\",\n",
    "            \"The {type} fiber as a conduit in the {tract} on the {side} side\",\n",
    "            \"The {type} fiber's architectural marvel within the {tract}\",\n",
    "            \"A journey alongside the {type} fiber through the {tract}\",\n",
    "            \"The harmonious structure of the {type} fiber in the {tract}\",\n",
    "            \"Unraveling the secrets of the {type} fiber in the {tract} tract\",\n",
    "            \"The {type} fiber: A key player in {tract} dynamics\",\n",
    "            \"Envisioning the {type} fiber's pathway in the {tract}\",\n",
    "            \"The strategic placement of the {type} fiber in the {tract}\",\n",
    "            \"Illuminating the {type} fiber's route through the {tract}\",\n",
    "            \"The {type} fiber: An essential bridge within the {tract}\",\n",
    "            \"Deciphering the network of {type} fibers in the {tract}\",\n",
    "            \"Exploring the synergy between {type} fibers and the {tract}\",\n",
    "            \"The {type} fiber's vital link in the neural network of the {tract}\"\n",
    "        ]\n",
    "\n",
    "\n",
    "    def get_caption(self, data: Data)-> Data:\n",
    "        # print(data.y) -> tensor([ 0,  0,  0,  ..., 29, 29, 29])\n",
    "\n",
    "        captions = []\n",
    "\n",
    "        for label in data.y:\n",
    "            info = self.TRACT_LIST[self.LABELS[label.item()]]\n",
    "            caption = random.choice(self.caption_templates).format(**info)\n",
    "            captions.append(caption)\n",
    "\n",
    "        if self.tokenize_data:\n",
    "            return self.tokenizer(captions, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        else:\n",
    "            return captions\n",
    "\n",
    "    def __call__(self, data: Data) -> Data:\n",
    "        \"\"\"\n",
    "        Add a caption to the data object.\n",
    "        \"\"\"\n",
    "        data.caption = self.get_caption(data)\n",
    "        return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
