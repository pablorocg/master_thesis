{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset de torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Compose, NormalizeFeatures\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_lightning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LightningDataModule\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Dataset\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Batch, Data\n",
    "import random\n",
    "from torch_geometric.transforms import Compose, NormalizeFeatures\n",
    "from transformers import AutoTokenizer\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import Batch as GeoBatch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "class MaxMinNormalization(BaseTransform):\n",
    "    def __init__(self, max_values=None, min_values=None):\n",
    "        \"\"\"\n",
    "        Initialize the normalization transform with optional max and min values.\n",
    "        If not provided, they should be computed from the dataset.\n",
    "        \"\"\"\n",
    "        self.max_values = max_values if max_values is not None else torch.tensor([76.03170776367188, 77.9359130859375, 88.72427368164062], dtype=torch.float)\n",
    "        self.min_values = min_values if min_values is not None else torch.tensor([-73.90082550048828, -112.23554992675781, -79.38320922851562], dtype=torch.float)\n",
    "\n",
    "    def __call__(self, data: Data) -> Data:\n",
    "        \"\"\"\n",
    "        Apply min-max normalization to the node features.\n",
    "        \"\"\"\n",
    "        data.x = (data.x - self.min_values) / (self.max_values - self.min_values)\n",
    "        return data\n",
    "    \n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "import random\n",
    "\n",
    "class AddCaptionTransform(BaseTransform):\n",
    "    def __init__(self, tokenize_data=True):\n",
    "        \"\"\"\n",
    "        Initialize the transform with the tokenizer and the tract list.\n",
    "        \"\"\"\n",
    "        self.tokenize_data = tokenize_data\n",
    "\n",
    "        if self.tokenize_data:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        else:\n",
    "            self.tokenizer = None\n",
    "\n",
    "\n",
    "        # Your TRACT_LIST and caption_templates can be defined here\n",
    "        self.TRACT_LIST = {\n",
    "            'AF_L': {\n",
    "                'id': 0,\n",
    "                'tract': 'arcuate fasciculus',\n",
    "                'side' : 'left',\n",
    "                'type': 'association'\n",
    "            },'AF_R': {\n",
    "                'id': 1, \n",
    "                'tract': 'arcuate fasciculus',\n",
    "                'side' : 'right',\n",
    "                'type': 'association'\n",
    "            },'CC_Fr_1': {\n",
    "                'id': 2, \n",
    "                'tract': 'corpus callosum, frontal lobe',\n",
    "                'side' : 'most anterior part of the frontal lobe', \n",
    "                'type': 'commissural'\n",
    "            },'CC_Fr_2': {\n",
    "                'id': 3, \n",
    "                'tract': 'corpus callosum, frontal lobe',\n",
    "                'side' : 'most posterior part of the frontal lobe',\n",
    "                'type': 'commissural'\n",
    "            },'CC_Oc': {\n",
    "                'id': 4, \n",
    "                'tract': 'corpus callosum, occipital lobe',\n",
    "                'side' : 'central',\n",
    "                'type': 'commissural'\n",
    "            },'CC_Pa': {\n",
    "                'id': 5, \n",
    "                'tract': 'corpus callosum, parietal lobe',\n",
    "                'side' : 'central',\n",
    "                'type': 'commissural'\n",
    "            },'CC_Pr_Po': {\n",
    "                'id': 6, \n",
    "                'tract': 'corpus callosum, pre/post central gyri',\n",
    "                'side' : 'central',\n",
    "                'type': 'commissural'\n",
    "            },'CG_L': {\n",
    "                'id': 7, \n",
    "                'tract': 'cingulum',\n",
    "                'side' : 'left',\n",
    "                'type': 'association'\n",
    "            },'CG_R': {\n",
    "                'id': 8,\n",
    "                'tract': 'cingulum',\n",
    "                'side' : 'right',\n",
    "                'type': 'association'\n",
    "            },'FAT_L': {\n",
    "                'id': 9,\n",
    "                'tract': 'frontal aslant tract',\n",
    "                'side' : 'left',\n",
    "                'type': 'association'\n",
    "            },'FAT_R': {\n",
    "                'id': 10,\n",
    "                'tract': 'frontal aslant tract',\n",
    "                'side' : 'right',\n",
    "                'type': 'association'\n",
    "            },'FPT_L': {\n",
    "                'id': 11,\n",
    "                'tract': 'fronto-pontine tract',\n",
    "                'side' : 'left',\n",
    "                'type': 'association' \n",
    "            },'FPT_R': {\n",
    "                'id': 12, \n",
    "                'tract': 'fronto-pontine tract',\n",
    "                'side' : 'right',\n",
    "                'type': 'association'\n",
    "            }, 'FX_L': {\n",
    "                'id': 13,\n",
    "                'tract': 'fornix',\n",
    "                'side' : 'left',\n",
    "                'type': 'commissural'\n",
    "            },'FX_R': {\n",
    "                'id': 14,\n",
    "                'tract': 'fornix',\n",
    "                'side' : 'right',\n",
    "                'type': 'commissural'\n",
    "            },'IFOF_L': {\n",
    "                'id': 15,\n",
    "                'tract': 'inferior fronto-occipital fasciculus',\n",
    "                'side' : 'left',\n",
    "                'type': 'association'\n",
    "            },'IFOF_R': {\n",
    "                'id': 16,\n",
    "                'tract': 'inferior fronto-occipital fasciculus',\n",
    "                'side' : 'right',\n",
    "                'type': 'association'\n",
    "            },'ILF_L': {\n",
    "                'id': 17,\n",
    "                'tract': 'inferior longitudinal fasciculus',\n",
    "                'side' : 'left',\n",
    "                'type': 'association'\n",
    "            },'ILF_R': {\n",
    "                'id': 18,\n",
    "                'tract': 'inferior longitudinal fasciculus',\n",
    "                'side' : 'right',\n",
    "                'type': 'association'\n",
    "            },'MCP': {\n",
    "                'id': 19,\n",
    "                'tract': 'middle cerebellar peduncle',\n",
    "                'side' : 'central',\n",
    "                'type': 'commissural'\n",
    "            },'MdLF_L': {\n",
    "                'id': 20,\n",
    "                'tract': 'middle longitudinal fasciculus',\n",
    "                'side' : 'left',\n",
    "                'type': 'association'\n",
    "            },'MdLF_R': {\n",
    "                'id': 21,\n",
    "                'tract': 'middle longitudinal fasciculus',\n",
    "                'side' : 'right',\n",
    "                'type': 'association'\n",
    "            },'OR_ML_L': {\n",
    "                'id': 22,\n",
    "                'tract': 'optic radiation, Meyer loop',\n",
    "                'side' : 'left',\n",
    "                'type': 'projection'\n",
    "            },'OR_ML_R': {\n",
    "                'id': 23,\n",
    "                'tract': 'optic radiation, Meyer loop',\n",
    "                'side' : 'right',\n",
    "                'type': 'projection'\n",
    "            },'POPT_L': {\n",
    "                'id': 24,\n",
    "                'tract': 'pontine crossing tract',\n",
    "                'side' : 'left',\n",
    "                'type': 'commissural'\n",
    "            },'POPT_R': {\n",
    "                'id': 25, \n",
    "                'tract': 'pontine crossing tract',\n",
    "                'side' : 'right',\n",
    "                'type': 'commissural'\n",
    "            },'PYT_L': {\n",
    "                'id': 26,\n",
    "                'tract': 'pyramidal tract',\n",
    "                'side' : 'left',\n",
    "                'type': 'projection' \n",
    "            },'PYT_R': {\n",
    "                'id': 27,\n",
    "                'tract': 'pyramidal tract',\n",
    "                'side' : 'right',\n",
    "                'type': 'projection' \n",
    "            },'SLF_L': {\n",
    "                'id': 28,\n",
    "                'tract': 'superior longitudinal fasciculus',\n",
    "                'side' : 'left',\n",
    "                'type': 'association' \n",
    "            },'SLF_R': {\n",
    "                'id': 29,\n",
    "                'tract': 'superior longitudinal fasciculus',\n",
    "                'side' : 'right',\n",
    "                'type': 'association' \n",
    "            },'UF_L': {\n",
    "                'id': 30,\n",
    "                'tract': 'uncinate fasciculus',\n",
    "                'side' : 'left',\n",
    "                'type': 'association'\n",
    "            },'UF_R': {\n",
    "                'id': 31,\n",
    "                'tract': 'uncinate fasciculus',\n",
    "                'side' : 'right',\n",
    "                'type': 'association'\n",
    "            }\n",
    "        }\n",
    "\n",
    "        self.LABELS = {value[\"id\"]: key for key, value in self.TRACT_LIST.items()}# Diccionario id -> Etiqueta\n",
    "\n",
    "        self.caption_templates = [\n",
    "            \"A {type} fiber\",\n",
    "            \"A {type} fiber on the {side} side\",\n",
    "            \"{type} fiber on the {side} side\",\n",
    "            \"A {type} fiber of the {tract}\",\n",
    "            \"{type} fiber of the {tract}\",\n",
    "            \"A {type} fiber of the {tract} on the {side} side\",\n",
    "            \"{type} fiber of the {tract} on the {side} side\",\n",
    "            \"{side} side\",\n",
    "            \"{tract} tract\",\n",
    "            \"{type} fiber\",\n",
    "            \"The {type} fiber located in the {tract} tract\",\n",
    "            \"This is a {type} fiber found on the {side} hemisphere\",\n",
    "            \"Detailed view of a {type} fiber within the {tract}\",\n",
    "            \"Observation of the {type} fiber, prominently on the {side} side\",\n",
    "            \"The {tract} tract's remarkable {type} fiber\",\n",
    "            \"Characteristics of a {type} fiber in the {tract} region\",\n",
    "            \"Notable {type} fiber on the {side} hemisphere of the {tract}\",\n",
    "            \"Insight into the {type} fiber's structure on the {side} side\",\n",
    "            \"Exploring the complexity of the {type} fiber in the {tract}\",\n",
    "            \"The anatomy of a {type} fiber on the {side} hemisphere\",\n",
    "            \"The {tract} tract featuring a {type} fiber\",\n",
    "            \"A comprehensive look at the {type} fiber, {side} orientation\",\n",
    "            \"A closer look at the {type} fiber's path in the {tract}\",\n",
    "            \"Unveiling the {type} fiber's role in the {tract} tract\",\n",
    "            \"Decoding the structure of the {type} fiber on the {side}\",\n",
    "            \"Highlighting the {type} fiber's significance in the {tract}\",\n",
    "            \"The {type} fiber: A journey through the {tract} on the {side}\",\n",
    "            \"A deep dive into the {type} fiber's dynamics in the {tract}\",\n",
    "            \"The {type} fiber's contribution to {tract} tract functionality\",\n",
    "            \"Mapping the {type} fiber's trajectory in the {tract} on the {side} side\",\n",
    "            \"Navigating the intricate pathways of the {type} fiber within the {tract}\",\n",
    "            \"The interplay of {type} fibers across the {side} hemisphere\",\n",
    "            \"Traversing the {tract} with a {type} fiber\",\n",
    "            \"The pivotal role of the {type} fiber in connecting the {tract}\",\n",
    "            \"Showcasing the unique texture of {type} fibers in the {tract}\",\n",
    "            \"Zooming in on the {type} fiber's impact on the {side} hemisphere\",\n",
    "            \"The {type} fiber in the {tract}\",\n",
    "            \"The {type} fiber as a conduit in the {tract} on the {side} side\",\n",
    "            \"The {type} fiber's architectural marvel within the {tract}\",\n",
    "            \"A journey alongside the {type} fiber through the {tract}\",\n",
    "            \"The harmonious structure of the {type} fiber in the {tract}\",\n",
    "            \"Unraveling the secrets of the {type} fiber in the {tract} tract\",\n",
    "            \"The {type} fiber: A key player in {tract} dynamics\",\n",
    "            \"Envisioning the {type} fiber's pathway in the {tract}\",\n",
    "            \"The strategic placement of the {type} fiber in the {tract}\",\n",
    "            \"Illuminating the {type} fiber's route through the {tract}\",\n",
    "            \"The {type} fiber: An essential bridge within the {tract}\",\n",
    "            \"Deciphering the network of {type} fibers in the {tract}\",\n",
    "            \"Exploring the synergy between {type} fibers and the {tract}\",\n",
    "            \"The {type} fiber's vital link in the neural network of the {tract}\"\n",
    "        ]\n",
    "\n",
    "\n",
    "    def get_caption(self, data: Data)-> Data:\n",
    "        # print(data.y) -> tensor([ 0,  0,  0,  ..., 29, 29, 29])\n",
    "\n",
    "        captions = []\n",
    "\n",
    "        for label in data.y:\n",
    "            info = self.TRACT_LIST[self.LABELS[label.item()]]\n",
    "            caption = random.choice(self.caption_templates).format(**info)\n",
    "            captions.append(caption)\n",
    "\n",
    "        if self.tokenize_data:\n",
    "            return self.tokenizer(captions, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        else:\n",
    "            return captions\n",
    "\n",
    "    def __call__(self, data: Data) -> Data:\n",
    "        \"\"\"\n",
    "        Add a caption to the data object.\n",
    "        \"\"\"\n",
    "        data.caption = self.get_caption(data)\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MyLazyDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, return_tokenized_text=True):\n",
    "        super(MyLazyDataset, self).__init__(root, transform, pre_transform)\n",
    "        # Tu código adicional de inicialización aquí (si es necesario)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "        self.return_tokenized_text = return_tokenized_text\n",
    "        self.transform = Compose([MaxMinNormalization(), AddCaptionTransform()])\n",
    "\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        # De manera similar, lista los archivos en el directorio 'processed'\n",
    "        return os.listdir(os.path.join(self.root, 'processed'))\n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        # Lista los archivos en el directorio 'raw'\n",
    "        return os.listdir(os.path.join(self.root, 'raw'))\n",
    "\n",
    "    def process(self):\n",
    "        # Aquí conviertes tus datos sin procesar en un formato procesado y los guardas en el directorio 'processed'\n",
    "        # Este método se llama después de 'download()' si es necesario\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def len(self):\n",
    "        # Devuelve el número de ejemplos en tu conjunto de datos\n",
    "        # Por ejemplo, puedes contar el número de archivos en 'processed'\n",
    "        return len(self.processed_file_names)\n",
    "    \n",
    "    def get(self, idx):\n",
    "        # Devuelve un ejemplo del conjunto de datos en el índice dado\n",
    "        # Necesitas cargar y devolver el ejemplo correspondiente de 'processed'\n",
    "        subject = self.processed_file_names[idx]# Seleccionar un sujeto\n",
    "\n",
    "        print('Cargando sujeto...')\n",
    "        graphs = torch.load(os.path.join(self.processed_dir, subject))\n",
    "        print('Sujeto cargado')\n",
    "\n",
    "        print('Aplicando transformaciones...')\n",
    "        if self.transform:\n",
    "            graphs = self.transform(graphs)\n",
    "        print('Transformaciones aplicadas')\n",
    "\n",
    "        return graphs\n",
    "\n",
    "    \n",
    "    # def norm_graph(self, graph:Data) -> Data:\n",
    "    #     \"\"\"\n",
    "    #     Normalizacion min-max con datos computados en el dataset de test (29 sujetos aprox 15M de fibras)\n",
    "    #     \"\"\"\n",
    "        \n",
    "    #     # Coordenadas x: \n",
    "    #     # Media: -0.13705343008041382\n",
    "    #     # Desviación típica: 46.49734878540039\n",
    "    #     # Máximo: 76.03170776367188\n",
    "    #     # Mínimo-73.90082550048828\n",
    "    #     # Coordenadas y: \n",
    "    #     # Media: -19.238847732543945\n",
    "    #     # Desviación típica: 60.372657775878906\n",
    "    #     # Máximo: 77.9359130859375\n",
    "    #     # Mínimo-112.23554992675781\n",
    "    #     # Coordenadas z: \n",
    "    #     # Media: 14.74081039428711\n",
    "    #     # Desviación típica: 50.90876388549805\n",
    "    #     # Máximo: 88.72427368164062\n",
    "    #     # Mínimo-79.38320922851562\n",
    "\n",
    "        \n",
    "\n",
    "    #     max = torch.tensor([76.03170776367188, 77.9359130859375, 88.72427368164062], dtype=torch.float)\n",
    "    #     min = torch.tensor([-73.90082550048828, -112.23554992675781, -79.38320922851562], dtype=torch.float)\n",
    "\n",
    "    #     graph.x = (graph.x - min) / (max - min)\n",
    "    #     return graph\n",
    "      \n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "# Uso de tu clase de conjunto de datos\n",
    "dataset = MyLazyDataset(root=r'C:\\Users\\pablo\\GitHub\\tfm_prg\\tractoinferno_graphs\\testset', return_tokenized_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando sujeto...\n",
      "Sujeto cargado\n",
      "Aplicando transformaciones...\n",
      "Transformaciones aplicadas\n"
     ]
    }
   ],
   "source": [
    "first_element = dataset[0] # Esto carga el primer ejemplo de tu conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataBatch(\n",
      "  x=[24849860, 3],\n",
      "  edge_index=[2, 49699720],\n",
      "  y=[837491],\n",
      "  batch=[24849860],\n",
      "  ptr=[837492],\n",
      "  caption={\n",
      "    input_ids=[837491, 32],\n",
      "    attention_mask=[837491, 32],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Ver el primer ejemplo de tu conjunto de datos\n",
    "print(first_element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando sujeto...\n",
      "Sujeto cargado\n",
      "DataBatch(x=[38768, 3], edge_index=[2, 77536], y=[1024], batch=[38768], ptr=[1025])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DataLoader.__init__() got an unexpected keyword argument 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m subject:\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[1;32m---> 31\u001b[0m     dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28mprint\u001b[39m(data)\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:98\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, follow_batch, exclude_keys, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, OnDiskDataset):\n\u001b[0;32m     96\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     99\u001b[0m     dataset,\n\u001b[0;32m    100\u001b[0m     batch_size,\n\u001b[0;32m    101\u001b[0m     shuffle,\n\u001b[0;32m    102\u001b[0m     collate_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollator\u001b[38;5;241m.\u001b[39mcollate_fn,\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    104\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: DataLoader.__init__() got an unexpected keyword argument 'transform'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "class MaxMinNormalization(BaseTransform):\n",
    "    def __init__(self, max_values=None, min_values=None):\n",
    "        \"\"\"\n",
    "        Initialize the normalization transform with optional max and min values.\n",
    "        If not provided, they should be computed from the dataset.\n",
    "        \"\"\"\n",
    "        self.max_values = max_values if max_values is not None else torch.tensor([76.03170776367188, 77.9359130859375, 88.72427368164062], dtype=torch.float)\n",
    "        self.min_values = min_values if min_values is not None else torch.tensor([-73.90082550048828, -112.23554992675781, -79.38320922851562], dtype=torch.float)\n",
    "\n",
    "    def __call__(self, data: Data) -> Data:\n",
    "        \"\"\"\n",
    "        Apply min-max normalization to the node features.\n",
    "        \"\"\"\n",
    "        data.x = (data.x - self.min_values) / (self.max_values - self.min_values)\n",
    "        return data\n",
    "\n",
    "    \n",
    "\n",
    "# Crear un transform customizado para generar captions\n",
    "transform = Compose([MaxMinNormalization()])\n",
    "\n",
    "# Crear un DataLoader que aplica el transform customizado\n",
    "for subject in dataset:\n",
    "    for batch in subject:\n",
    "        print(batch)\n",
    "        dataloader = DataLoader(batch, batch_size=32, shuffle=True, transform=transform)\n",
    "        for data in dataloader:\n",
    "            print(data)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Formato del dataset\n",
    "\n",
    "# grafos (lista de Data), captions (lista de textos)\n",
    "\n",
    "\n",
    "# Codigo para generar un batch \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando sujeto...\n",
      "Sujeto cargado\n",
      "Aplicando normalización...\n",
      "Normalizacion aplicada\n",
      "Generando captions...\n",
      "Captions generadas\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'stores'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch_geometric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataListLoader, DataLoader\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m subject \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(DataLoader(subject, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(batch)\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:55\u001b[0m, in \u001b[0;36mCollater.collate_fn\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, OnDiskDataset):\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mmulti_get(batch))\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:48\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(elem)(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader found invalid type: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(elem)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:48\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(elem)(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)))\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch)]\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader found invalid type: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(elem)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:28\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     26\u001b[0m elem \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, BaseData):\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_data_list\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch_geometric\\data\\batch.py:93\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[1;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_data_list\u001b[39m(\u001b[38;5;28mcls\u001b[39m, data_list: List[BaseData],\n\u001b[0;32m     83\u001b[0m                    follow_batch: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     84\u001b[0m                    exclude_keys: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     85\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m    Python list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m    :obj:`follow_batch`.\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 93\u001b[0m     batch, slice_dict, inc_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincrement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_num_graphs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data_list)\n\u001b[0;32m    103\u001b[0m     batch\u001b[38;5;241m.\u001b[39m_slice_dict \u001b[38;5;241m=\u001b[39m slice_dict\n",
      "File \u001b[1;32mc:\\Users\\pablo\\anaconda3\\envs\\tfm_prg\\lib\\site-packages\\torch_geometric\\data\\collate.py:54\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     52\u001b[0m key_to_stores \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m data_list:\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m store \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstores\u001b[49m:\n\u001b[0;32m     55\u001b[0m         key_to_stores[store\u001b[38;5;241m.\u001b[39m_key]\u001b[38;5;241m.\u001b[39mappend(store)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# With this, we iterate over each list of storage objects and recursively\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# collate all its attributes into a unified representation:\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m#   elements as attributes that got incremented need to be decremented\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m#   while separating to obtain original values.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'stores'"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataListLoader, DataLoader\n",
    "\n",
    "# def custom_collate_fn(batch):\n",
    "#     graphs = [item['graph'] for item in batch]\n",
    "#     input_ids = [item['text']['input_ids'].squeeze(0) for item in batch]\n",
    "#     attention_masks = [item['text']['attention_mask'].squeeze(0) for item in batch]\n",
    "#     padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "#     padded_attention_masks = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "#     batched_graphs = GeoBatch.from_data_list(graphs)\n",
    "#     return batched_graphs, {'input_ids': padded_input_ids, 'attention_mask': padded_attention_masks}\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GCNConv, BatchNorm\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class GraphClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(GraphClassifier, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 16)\n",
    "        self.conv2 = GCNConv(16, 32)\n",
    "        self.conv3 = GCNConv(32, 256)\n",
    "        self.conv4 = GCNConv(256, 512)\n",
    "        self.fc1 = torch.nn.Linear(512, 256)\n",
    "        self.fc2 = torch.nn.Linear(256, 64)\n",
    "        self.fc3 = torch.nn.Linear(64, 32)\n",
    "        self.fc = torch.nn.Linear(32, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        x = BatchNorm(x.size()[1])(x)\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = BatchNorm(x.size()[1])(x)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = BatchNorm(x.size()[1])(x)\n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = BatchNorm(x.size()[1])(x)\n",
    "        x = F.relu(self.conv4(x, edge_index))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = BatchNorm(x.size()[1])(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = BatchNorm(x.size()[1])(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = BatchNorm(x.size()[1])(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = BatchNorm(x.size()[1])(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = GraphClassifier(3, 32).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "    for subject in dataset:\n",
    "        for batch in tqdm(DataLoader(subject, batch_size=128, shuffle=True)):\n",
    "            data = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data)\n",
    "            loss = F.nll_loss(out, data.y)\n",
    "            # print(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Epoch: {:03d}, Loss: {:.5f}'.format(epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterar sobre el conjunto de datos\n",
    "for data in dataset:\n",
    "    for batch in DataListLoader(data, batch_size=128, shuffle=True):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader de torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "for batch in loader:\n",
    "    print(batch.shape)\n",
    "    print(batch.num_graphs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
